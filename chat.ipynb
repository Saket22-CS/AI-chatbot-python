{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7767d6b0",
   "metadata": {},
   "source": [
    "# ü§ñ Build a Chatbot Using Python & Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960b70f",
   "metadata": {},
   "source": [
    "## üß∞ Step 1: Install Required Libraries\n",
    "pip install \n",
    "tensorflow==2.3.1 \n",
    "nltk==3.5 \n",
    "colorama==0.4.3 \n",
    "numpy==1.18.5 \n",
    "scikit_learn==0.23.2 \n",
    "Flask==1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98557156",
   "metadata": {},
   "source": [
    "## üìÑ Step 2: Create the Intents JSON File Create a file named `intents.json`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7ad5f",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 3: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033dd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fcd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "responses = []\n",
    "\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        training_sentences.append(pattern)\n",
    "        training_labels.append(intent['tag'])\n",
    "    responses.append(intent['responses'])\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea2ee7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_encoder = LabelEncoder()\n",
    "lbl_encoder.fit(training_labels)\n",
    "training_labels = lbl_encoder.transform(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e2f9e",
   "metadata": {},
   "source": [
    "## üßæ Step 4: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e881619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_len = 20\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f652c6e",
   "metadata": {},
   "source": [
    "## üß† Step 5: Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs = 500\n",
    "history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfc1e5",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Save the Model and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a46bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the trained model\n",
    "model.save(\"chat_model.keras\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# to save the fitted tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted label encoder\n",
    "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75fcb8",
   "metadata": {},
   "source": [
    "## üí¨ Step 7: Chat with the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf99225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start messaging with the bot (type quit to stop)!\n",
      "User: ChatBot: Hi there\n",
      "User: ChatBot: Yes Sure, How can I support you\n",
      "User: ChatBot: I.m Joana, your bot assistant\n",
      "User: ChatBot: Tell me your problem to assist you\n",
      "User: "
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import colorama \n",
    "colorama.init()\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    # load trained model\n",
    "    # >>> FIX THIS LINE <<<\n",
    "    model = keras.models.load_model('chat_model.keras') # Use the correct extension (.keras or .h5)\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    \n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        # Ensure pad_sequences is imported or fully qualified\n",
    "        # It's already correctly referenced as keras.preprocessing.sequence.pad_sequences\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
    "                                                                          truncating='post', maxlen=max_len), verbose=0)\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])[0] # Get the single tag string\n",
    "\n",
    "        found_response = False\n",
    "        for i in data['intents']:\n",
    "            if i['tag'] == tag:\n",
    "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
    "                found_response = True\n",
    "                break\n",
    "        if not found_response:\n",
    "            print(Fore.RED + \"ChatBot: \" + Style.RESET_ALL + \"Sorry, I don't understand that.\")\n",
    "\n",
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
    "chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
